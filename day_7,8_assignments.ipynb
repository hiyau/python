{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f4yi9B0rEXF",
        "outputId": "d4cef6c2-7739-42e4-ad96-2e7eefed1752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves a variety of techniques, such as tokenization, stemming, and lemmatization, to process and analyze text data efficiently. By understanding the context and semantics of language, NLP enables machines to perform tasks like language translation, sentiment analysis, and information retrieval.\n",
            "\n",
            "\n",
            "Processed Tokens:\n",
            "['natur', 'languag', 'process', 'nlp', 'branch', 'artifici', 'intellig', 'focus', 'interact', 'comput', 'human', 'natur', 'languag', 'involv', 'varieti', 'techniqu', 'token', 'stem', 'lemmat', 'process', 'analyz', 'text', 'data', 'effici', 'understand', 'context', 'semant', 'languag', 'nlp', 'enabl', 'machin', 'perform', 'task', 'like', 'languag', 'translat', 'sentiment', 'analysi', 'inform', 'retriev']\n",
            "Processed tokens have been saved to 'processed_tokens.txt'.\n"
          ]
        }
      ],
      "source": [
        "#day 7 assignment\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text using Gensim's simple_preprocess\n",
        "    tokens = simple_preprocess(text, deacc=True)  # deacc=True removes punctuations\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Apply lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Load text data from a file\n",
        "input_file = \"/content/sample_text.txt\"\n",
        "try:\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{input_file}' was not found.\")\n",
        "    text_data = \"\"  # Placeholder for an empty string\n",
        "\n",
        "# Preprocess the text\n",
        "processed_tokens = preprocess_text(text_data)\n",
        "\n",
        "# Output the results\n",
        "print(\"Original Text:\")\n",
        "print(text_data)\n",
        "print(\"\\nProcessed Tokens:\")\n",
        "print(processed_tokens)\n",
        "\n",
        "# Save the processed tokens to a file\n",
        "output_file = \"processed_tokens.txt\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(\" \".join(processed_tokens))\n",
        "\n",
        "print(f\"Processed tokens have been saved to '{output_file}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day8 assignment\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# Define a sample paragraph\n",
        "sample_paragraph = \"Natural Language Processing is a fascinating field of AI. It deals with the interaction between computers and humans using natural language. Tokenization is one of its fundamental steps.\"\n",
        "\n",
        "# Tokenize the paragraph into sentences\n",
        "sentences = sent_tokenize(sample_paragraph)\n",
        "\n",
        "# Tokenize the paragraph into words\n",
        "words = word_tokenize(sample_paragraph)\n",
        "\n",
        "# Output the results\n",
        "print(\"Original Paragraph:\")\n",
        "print(sample_paragraph)\n",
        "print(\"\\nTokenized Sentences:\")\n",
        "print(sentences)\n",
        "print(\"\\nTokenized Words:\")\n",
        "print(words)\n",
        "\n",
        "# Save the tokenized sentences and words to files\n",
        "with open(\"tokenized_sentences.txt\", \"w\", encoding=\"utf-8\") as sent_file:\n",
        "    sent_file.write(\"\\n\".join(sentences))\n",
        "\n",
        "with open(\"tokenized_words.txt\", \"w\", encoding=\"utf-8\") as word_file:\n",
        "    word_file.write(\" \".join(words))\n",
        "\n",
        "print(\"Tokenized sentences have been saved to 'tokenized_sentences.txt'.\")\n",
        "print(\"Tokenized words have been saved to 'tokenized_words.txt'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17rVABmqrdfM",
        "outputId": "6e2959f4-e115-4e23-d7df-98c6c588bb34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph:\n",
            "Natural Language Processing is a fascinating field of AI. It deals with the interaction between computers and humans using natural language. Tokenization is one of its fundamental steps.\n",
            "\n",
            "Tokenized Sentences:\n",
            "['Natural Language Processing is a fascinating field of AI.', 'It deals with the interaction between computers and humans using natural language.', 'Tokenization is one of its fundamental steps.']\n",
            "\n",
            "Tokenized Words:\n",
            "['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'Tokenization', 'is', 'one', 'of', 'its', 'fundamental', 'steps', '.']\n",
            "Tokenized sentences have been saved to 'tokenized_sentences.txt'.\n",
            "Tokenized words have been saved to 'tokenized_words.txt'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    }
  ]
}